---
title: "Finding Most Accurate Words"
author: "morgan moyer"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE, warning=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(lme4)
library(lmerTest)
library(multcomp) # not available for this version of R
library(stringr)
library(textstem)
library(tidyverse)
library(kableExtra)
theme_set(theme_bw())
cbPalette <- c("#56B4E9", "#D55E00", "#009E73","#999999", "#E69F00","#009E73","#56B4E9", "#D55E00", "#009E73","#999999", "#E69F00","#009E73","#56B4E9", "#D55E00", "#009E73","#999999", "#E69F00","#009E73","#56B4E9", "#D55E00", "#009E73","#999999", "#E69F00","#009E73")
this.dir <- dirname(rstudioapi::getSourceEditorContext()$path)
setwd(this.dir)
source("../../../helpers.R")

p1 <- read.csv("../data/p1_processed.csv")
p1["Version"] <- "Pilot1"
p1["WhoseList"] <- "Morgan1"

p2 <- read.csv("../data/p2_corrected.csv")
p2["Version"] <- "Pilot2"
p2["WhoseList"] <- "Morgan2"
p2$Accuracy <- NULL # delete original accuracy column
colnames(p2)[colnames(p2) == "CorrectedAccuracy"] <- "Accuracy"

p3 <- read.csv("../data/p3_processed.csv")
p3["Version"] <- "Pilot3"


d <- bind_rows(p1,p2,p3)

d$Task <- gsub("Semantic", "Concrete", d$Task)


morph <- read_csv('../data/morpholex_one_derivational_suffix.csv')
nosuffix <- read_csv('../data/morpholex_no_suffix.csv')

```

```{r}
# Set the maximum width for printing all columns in the console
# Ensure that the full tibble output is printed
options(tibble.print_max = Inf, tibble.print_min = Inf, tibble.width = Inf)
```


# First, did one subject take the study twice?
5ee0e5063f7f190b2b3f3b14
```{r}
d.s <- d %>% 
  filter(ID.true == "5ee0e5063f7f190b2b3f3b14")

table(d.s$BlockOrder)
# yeah this person took the study twice


# Are there any other participants that took the test twice?
# answer is No:

ids_with_both_block_orders <- d %>%
  filter(Version %in% c("Pilot2","Pilot3")) %>% 
  group_by(ID.true) %>%
  filter(all(c("CV", "VC") %in% BlockOrder)) %>%
  distinct(ID.true) %>%  # Get unique `ID.true` values that meet the condition
  pull(ID.true)          # Extract `ID.true` as a vector


length(ids_with_both_block_orders)
```


# Removing morphologically complex words
- concrete-negative: discourage
- abstract-negative: dislike, despise, abhor

```{r}
# Filter for words starting with "ab" or "dis"
words_with_prefixes <- d %>%
  filter(str_starts(Word, "ab") | str_starts(Word, "dis") | str_starts(Word, "de")) %>%
  select(Word) %>%
  distinct()

# Print the result
print(words_with_prefixes)
```



```{r}
# Filter those bad words out
d.conc <- d %>% 
  # filter(Word %in% words_with_multiple_ConcValCombo$Word) %>% 
  filter(Task == "Concrete") %>% 
  filter(!Word %in% words_with_prefixes$Word) %>% 
  mutate(Response.n = as.numeric(factor(Response, levels = c("abstract", "concrete"))) - 1) # Convert to numeric and subtract 1


# Filter those bad words out
d.val <- d %>% 
  # filter(Word %in% words_with_multiple_ConcValCombo$Word) %>% 
  filter(Task == "Valence") %>% 
  filter(!Word %in% words_with_prefixes$Word) %>% 
  mutate(Response.n = as.numeric(factor(Response, levels = c("negative", "positive"))) - 1) # Convert to numeric and subtract 1
```

```{r}
  
agr <- d.conc %>% 
  group_by(Word) %>%
  summarize(PropConcrete = mean(Response.n))
  # select(ID.true, Task,ConcValCombo, Word, WhoseList, Response)

dodge = position_dodge(.9)
ggplot(data=agr, aes(x=reorder(Word,PropConcrete),y=PropConcrete,fill=Word)) +
  geom_bar(position=dodge,stat="identity") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  guides(fill = "none")
```




## Only words with PropConcrete below .25 (=abstract) or above .75 (=concrete)
```{r}
conc <- d.conc %>% 
  # filter(!Word %in% words_with_prefixes$Word) %>% 
  group_by(Word,ConcValCombo) %>%
  summarize(PropConcrete = mean(Response.n)) %>% 
  filter(PropConcrete > .75 | PropConcrete < .25)

dodge = position_dodge(.9)
ggplot(data=conc, aes(x=reorder(Word,PropConcrete),y=PropConcrete,fill=ConcValCombo)) +
  geom_bar(position=dodge,stat="identity") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  guides(fill = "none")

table(conc$ConcValCombo)

table(d.conc[d.conc$Word %in% conc$Word,]$ConcValCombo)
```

```{r}
d.conc %>%
  # filter(!Word %in% words_with_prefixes$Word) %>%
  filter(Word %in% conc$Word) %>%
  group_by(ConcValCombo) %>%
  summarise(Word_List = paste(unique(Word), collapse = ", ")) %>%
  pivot_wider(names_from = ConcValCombo, values_from = Word_List) %>%
  knitr::kable() %>% 
  kable_styling(full_width = TRUE, position = "center", bootstrap_options = c("striped", "hover"))

d.conc <- d.conc %>% 
  filter(Word %in% conc$Word)
```

```{r}
dd.val <- d %>% 
  filter(Task == "Valence") %>% 
  filter(Word %in% conc$Word) %>% 
  group_by(Word,ConcValCombo) %>%
  mutate(Response.n = as.numeric(factor(Response, levels = c("negative", "positive"))) - 1) %>%  # Convert to numeric and subtract 1
  summarize(PropPositive = mean(Response.n)) %>% 
  filter(PropPositive > .1 | PropPositive < .9)

dodge = position_dodge(.9)
ggplot(data=dd.val, aes(x=reorder(Word,PropPositive),y=PropPositive,fill=ConcValCombo)) +
  geom_bar(position=dodge,stat="identity") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  guides(fill = "none")

dd.val %>%
  group_by(ConcValCombo) %>%
  summarise(Word_List = paste(unique(Word), collapse = ", ")) %>%
  pivot_wider(names_from = ConcValCombo, values_from = Word_List) %>%
  knitr::kable() %>% 
  kable_styling(full_width = TRUE, position = "center", bootstrap_options = c("striped", "hover"))
  
```


# Multiple ConcValCombo Words, why?

Is its better to remove cases of dispute rather than cases with the higher difference in means??
```{r}
# Identify words with multiple ConcValCombo values
words_with_multiple_ConcValCombo <- dd.val %>%
  group_by(Word) %>%
  summarise(ConcValCombo_count = n_distinct(ConcValCombo)) %>%  # Count unique ConcValCombo values per Word
  filter(ConcValCombo_count > 1) %>%                           # Keep only Words with multiple ConcValCombo values
  pull(Word)                                                   # Extract the list of Words

# Print the result
print(words_with_multiple_ConcValCombo)

sdf <- d[d$Word %in% words_with_multiple_ConcValCombo,]

```

## Concrete has some middle values that need recoding: 
(no worries about valence so won't graph)

```{r}

d.test.conc <- d %>% 
  filter(Word %in% words_with_multiple_ConcValCombo) %>% 
  filter(Task == "Concrete") %>%
  mutate(Response.n = as.numeric(factor(Response, levels = c("abstract", "concrete"))) - 1) # Convert to numeric and subtract 1

agr <- d.test.conc %>% 
  group_by(Word) %>%
  summarize(PropConcrete = mean(Response.n))
  # select(ID.true, Task,ConcValCombo, Word, WhoseList, Response)

dodge = position_dodge(.9)
ggplot(data=agr, aes(x=reorder(Word,PropConcrete),y=PropConcrete,fill=Word)) +
  geom_bar(position=dodge,stat="identity")

```

## Need to first recode
- vioate = concrete (according to participant responses)
- hope = concrete (also according to participant responses)


```{r}
d.final <- d %>%
  mutate(ConcValCombo = case_when(
    Word == "hope" ~ "concrete-positive",
    Word == "wish" ~ "concrete-positive",
    Word == "caress" ~ "concrete-positive",
    Word == "violate" ~ "concrete-negative",
    Word == "envy" ~ "abstract-negative",
    Word == "irritate" ~ "abstract-negative",
    TRUE ~ ConcValCombo  # Keep original value if condition is not met
  ))

# Test to confirm changes
d.final %>%
  filter(Word %in% c("hope", "wish", "caress", "violate", "envy", "irritate")) %>%
  select(Word, ConcValCombo) %>%
  unique() %>% 
  print()

table(d.final$ConcValCombo)

d.final <- d.final %>% 
  filter(Word %in% dd.val$Word)
```


# TRY WITH With total data set REAL GOON

There some ordering issues here, we need to run the concval combo chunk before this

```{r}

d <- d %>%
  mutate(Word = ifelse(Word == "embellish", "embelish", Word))
# Step 1: Pivot wider with Response columns
dd.conc <- d %>% 
  filter(!Word %in% words_with_prefixes$Word ) %>% 
  filter(Task == "Concrete") %>% 
  rename(ConcreteResponse = Response) %>%
  mutate(
    # Convert categorical responses to numeric binary values, handling NAs
    ConcreteResponse.n = as.numeric(factor(ConcreteResponse, levels = c("abstract", "concrete"))) - 1
  ) %>% 
  group_by(Word) %>% 
  summarize( PropConcrete = mean(ConcreteResponse.n, na.rm = TRUE))

dd.val <- d %>% 
  filter(!Word %in% words_with_prefixes$Word ) %>% 
  filter(Task == "Valence") %>% 
  rename(PositiveResponse = Response) %>%
  mutate(
    # Convert categorical responses to numeric binary values, handling NAs
    PositiveResponse.n = as.numeric(factor(PositiveResponse, levels = c("negative", "positive"))) - 1
  ) %>% 
  group_by(Word) %>% 
  summarize(PropPositive = mean(PositiveResponse.n, na.rm = TRUE))

dd.fin <- inner_join(dd.val,dd.conc, by=c("Word"))

dd.fin <- dd.fin %>% 
  mutate(
    # Assign ConcValStrict based on proportions
    ConcValStrict = case_when(
      PropConcrete >= 0.75 & PropPositive >= 0.75 ~ "concrete-positive",
      PropConcrete >= 0.75 & PropPositive <= 0.25 ~ "concrete-negative",
      PropConcrete <= 0.25 & PropPositive >= 0.75 ~ "abstract-positive",
      PropConcrete <= 0.25 & PropPositive <= 0.25 ~ "abstract-negative",
      TRUE ~ NA_character_
    )
  )


dd.fin %>%
  group_by(ConcValStrict) %>%
  summarise(Word_List = paste(Word, collapse = ", ")) %>%
  pivot_wider(names_from = ConcValStrict, values_from = Word_List) %>%
  knitr::kable() %>% 
  kable_styling(full_width = TRUE, position = "center", bootstrap_options = c("striped", "hover"))

View(dd.fin)

# View final result
table(dd.fin$ConcValStrict)

# View(dd.fin)

dd.fin.fin <- dd.fin %>% 
  filter(!is.na(ConcValStrict))

# View final result
table(dd.fin.fin$ConcValStrict)

dd.fin.fin %>%
  group_by(ConcValStrict) %>%
  summarise(Word_List = paste(Word, collapse = ", ")) %>%
  pivot_wider(names_from = ConcValStrict, values_from = Word_List) %>%
  knitr::kable() %>% 
  kable_styling(full_width = TRUE, position = "center", bootstrap_options = c("striped", "hover"))


```


```{r,fig.width=15, fig.height=5}
dodge = position_dodge(.9)
ggplot(data=dd.fin.fin, aes(x=reorder(Word,PropConcrete),y=PropConcrete,fill=ConcValStrict)) +
  geom_bar(position=dodge,stat="identity") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r,fig.width=15, fig.height=5}
dodge = position_dodge(.9)
ggplot(data=dd.fin.fin, aes(x=reorder(Word,PropPositive),y=PropPositive,fill=ConcValStrict)) +
  geom_bar(position=dodge,stat="identity") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```



# Look at Word Length
Remove words outside 1sd of the mean


```{r}
dd.fin.fin <- dd.fin.fin %>%
  mutate(WordLength = nchar(Word))


# Plot the distribution of word lengths
ggplot(dd.fin.fin, aes(x = WordLength)) +
  geom_histogram(binwidth = 1, fill = "skyblue", color = "black") +
  labs(title = "Distribution of Word Lengths",
       x = "Word Length (Number of Characters)",
       y = "Frequency") +
  theme_minimal()
length(unique(dd.fin.fin$Word))



upper.lim <- mean(dd.fin.fin$WordLength) + 2*sd(dd.fin.fin$WordLength)
lower.lim <- mean(dd.fin.fin$WordLength) - 2*sd(dd.fin.fin$WordLength)



long.words <- dd.fin.fin %>% 
  filter(WordLength > upper.lim)

# Plot the distribution of word lengths
ggplot(long.words, aes(x = WordLength)) +
  geom_histogram(binwidth = 1, fill = "skyblue", color = "black") +
  labs(title = "Distribution of Word Lengths",
       x = "Word Length (Number of Characters)",
       y = "Frequency") +
  theme_minimal()

table(long.words$ConcValStrict)
print(long.words$Word)

just.right <- dd.fin.fin %>% 
  filter(WordLength < upper.lim & WordLength > lower.lim)

# Plot the distribution of word lengths
ggplot(just.right, aes(x = WordLength)) +
  geom_histogram(binwidth = 1, fill = "skyblue", color = "black") +
  labs(title = "Distribution of Word Lengths",
       x = "Word Length (Number of Characters)",
       y = "Frequency") +
  theme_minimal()

length(unique(just.right$Word))
table(just.right$ConcValStrict)

summary(just.right$WordLength)

```


## Include only words within the right range
```{r}
d.fin.fin.just.right <- dd.fin.fin %>% 
  filter(Word %in% just.right$Word)

table(d.fin.fin.just.right$ConcValStrict)

View(d.final)
```




```{r}
d.fin.fin.just.right %>%
  group_by(ConcValStrict) %>%
  summarise(Word_List = paste(Word, collapse = ", ")) %>%
  pivot_wider(names_from = ConcValStrict, values_from = Word_List) %>%
  knitr::kable() %>% 
  kable_styling(full_width = TRUE, position = "center", bootstrap_options = c("striped", "hover"))


```


```{r}
View(d)
dfacts <- d[,c("ID.true", "Word", "Task","Response", "ReactionTime", "LogReactionTime")]


diff <- d.fin.fin.just.right %>% 
  inner_join(dfacts, by = "Word")


diff$Accuracy <- mapply(function(response, pattern) {
  ifelse(grepl(response, pattern), 1, 0)
}, diff$Response, diff$ConcValStrict)


difft <- diff %>% 
  filter(!is.na(Accuracy)) %>%  # Remove rows with missing Accuracy
  group_by(Word, Task) %>%
  summarise(MeanAccuracy = mean(Accuracy)) %>%  # Calculate mean Accuracy for each group
  spread(Task, MeanAccuracy) %>%
  # spread(key = ConcValCombo, value = MeanAccuracy) %>%  # Spread the data so Concrete and Valence are separate columns
  mutate(Difference = abs(Concrete - Valence)) %>%  # Calculate the absolute difference
  arrange(Difference) 


diff <- inner_join(difft, diff, by ="Word")



```


```{r,fig.width=15, fig.height=5}
dodge = position_dodge(.9)
ggplot(data=diff, aes(x=reorder(Word,Difference),y=Difference,fill=ConcValStrict)) +
  geom_bar(position=dodge,stat="identity") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


table(d.fin.fin.just.right$ConcValStrict)
```




# Final Move

filter out the :
- 4 abs-neg  with max diff
- 8 abstract-positive
- 2 concrete-negative
- 1 concrete-positive
```{r}
names(diff)


# 
max.diff.pos.conc <- diff %>%
  # Rank the rows based on Difference within each ConcValStrict group
  group_by(ConcValStrict) %>%
  mutate(
    rank_difference = dense_rank(desc(Difference))  # Ranking Difference in descending order
  ) %>%
  # Create max_diff column within each group
  mutate(max_diff = max(Difference, na.rm = TRUE)) %>%
  # Filter out the row where ConcValStrict == "concrete-positive" and Difference equals the max_diff
  filter(
    (ConcValStrict == "concrete-positive" & Difference == max_diff)
  ) 

# View(max.diff.pos.conc)

# Remove the word forgive because its prop conc is lower than the other one

finale <- diff %>% 
  filter(!Word == "forgive") %>% 
  group_by(ConcValStrict) %>%
  mutate(
    # Ranking rows within each ConcValStrict group by Difference
    rank_difference = dense_rank(desc(Difference))  
  ) %>%
  ungroup() %>%  # Remove grouping after calculation
  # Check how many rows are left after filtering (for debugging)
  filter(
    # Filtering based on rank_difference for each ConcValStrict category
    (ConcValStrict == "abstract-negative" & rank_difference > 3) |
    (ConcValStrict == "abstract-positive" & rank_difference > 8) |
    (ConcValStrict == "concrete-negative" & rank_difference > 2) |
    (ConcValStrict == "concrete-positive")  # Allow all rows for "concrete-positive"
  ) %>%
  distinct(Word, ConcValStrict)

table(finale$ConcValStrict)
View(wordlist)

table(finale$ConcValStrict)
```




```{r}
finale %>%
  group_by(ConcValStrict) %>%
  summarise(Word_List = paste(unique(Word), collapse = ", ")) %>%
  pivot_wider(names_from = ConcValStrict, values_from = Word_List) %>%
  knitr::kable() %>% 
  kable_styling(full_width = TRUE, position = "center", bootstrap_options = c("striped", "hover"))

# write.csv(finale,"../data/pilot4_words.csv")

```


# Counterbalance

Valence
```{r}

assign_values <- function(df) {
  # Create Group, F_value, and J_value columns if they do not exist
  if (!"Group" %in% colnames(df)) {
    df$Group <- "A"  # Default value for Group if it doesn't exist
  }
  if (!"F_value" %in% colnames(df)) {
    df$F_value <- NA  # Initialize F_value with NA if it doesn't exist
  }
  if (!"J_value" %in% colnames(df)) {
    df$J_value <- NA  # Initialize J_value with NA if it doesn't exist
  }
  
  # Create the first copy with Group == "A", F_value == "negative", J_value == "positive"
  df_A <- df %>%
    mutate(
      Group = "A",  # Set Group to A
      F_value = "negative",  # Set F_value to negative
      J_value = "positive"  # Set J_value to positive
    )
  
  # Create the second copy with Group == "B", F_value == "positive", J_value == "negative"
  df_B <- df %>%
    mutate(
      Group = "B",  # Set Group to B
      F_value = "positive",  # Set F_value to positive
      J_value = "negative"  # Set J_value to negative
    )
  
  # Combine both copies together
  df_combined <- bind_rows(df_A, df_B)
  
  return(df_combined)
}



p4.val <- assign_values(finale)

View(p4.val)

write.csv(p4.val,"../data/pilot4_valance.csv")

```




```{r}


assign_values <- function(df) {
  # Create Group, F_value, and J_value columns if they do not exist
  if (!"Group" %in% colnames(df)) {
    df$Group <- "A"  # Default value for Group if it doesn't exist
  }
  if (!"F_value" %in% colnames(df)) {
    df$F_value <- NA  # Initialize F_value with NA if it doesn't exist
  }
  if (!"J_value" %in% colnames(df)) {
    df$J_value <- NA  # Initialize J_value with NA if it doesn't exist
  }
  
  # Create the first copy with Group == "A", F_value == "negative", J_value == "positive"
  df_A <- df %>%
    mutate(
      Group = "A",  # Set Group to A
      F_value = "abstract",  # Set F_value to negative
      J_value = "concrete"  # Set J_value to positive
    )
  
  # Create the second copy with Group == "B", F_value == "positive", J_value == "negative"
  df_B <- df %>%
    mutate(
      Group = "B",  # Set Group to B
      F_value = "concrete",  # Set F_value to positive
      J_value = "abstract"  # Set J_value to negative
    )
  
  # Combine both copies together
  df_combined <- bind_rows(df_A, df_B)
  
  return(df_combined)
}


p4.conc <- assign_values(finale)

View(p4.conc)

write.csv(p4.conc,"../data/pilot4_concrete.csv")

```




```{r}
d.final.final <- d.final.final %>%
  pivot_wider(
    names_from = Task,
    values_from = Response,
    names_prefix = "Response",
    values_fn = list(Response = mean)  # Replace with the desired aggregation function
  ) %>%
  rename(
    ConcreteResponse = ResponseConcrete,
    PositiveResponse = ResponseValence
  ) %>%
  mutate(
    ConcreteResponse.n = as.numeric(factor(ConcreteResponse, levels = c("abstract", "concrete"))) - 1,
    PositiveResponse.n = as.numeric(factor(PositiveResponse, levels = c("negative", "positive"))) - 1
  )

  
agr <- d.final.final %>% 
  group_by(Word) %>% 
  mutate(PropConcrete = mean(ConcreteResponse.n),
         PropPositive = mean(PositiveResponse.n)) %>% 
  mutate(ConcValStrict = case_when(
    PropConcrete > 0.8 & PropPositive > 0.8 ~ "concrete-positive",
    PropConcrete > 0.8 & PropPositive < 0.2 ~ "concrete-negative",
    PropConcrete < 0.2 & PropPositive > 0.8 ~ "abstract-positive",
    PropConcrete < 0.2 & PropPositive < 0.2 ~ "abstract-negative",
    TRUE ~ NA_character_  # Use NA or any other default value if conditions are not met
  ))

View(d.final.final$ConcValStrict)

# Redo the accuracy column given that we've recoded concvalcombo
d.final$Accuracy <- mapply(function(response, pattern) {
  ifelse(grepl(response, pattern), 1, 0)
}, d.final$Response, d.final$ConcValStrict)
length(unique(d.final$Word))

```



```{r}

names(d.final)
d.final.final <- d.final[,c("ID.true","Word","Task","Response","ReactionTime","LogReactionTime","Version","WhoseList","WordLength")]

View(d.final.final)

# d.final.final %>%
#   summarise(n = n(), .by = c(ID.true, Word, ReactionTime, LogReactionTime, Version, WhoseList, WordLength, Task)) %>%
#   filter(n > 1L)


# Step 1: Pivot wider with Response columns
d.final.conc <- d.final %>% 
  filter(Task == "Concrete") %>% 
  rename(ConcreteResponse = Response) %>%
  mutate(
    # Convert categorical responses to numeric binary values, handling NAs
    ConcreteResponse.n = as.numeric(factor(ConcreteResponse, levels = c("abstract", "concrete"))) - 1
  ) %>% 
  group_by(Word) %>% 
  summarize( PropConcrete = mean(ConcreteResponse.n, na.rm = TRUE))

d.final.val <- d.final %>% 
  filter(Task == "Valence") %>% 
  rename(PositiveResponse = Response) %>%
  mutate(
    # Convert categorical responses to numeric binary values, handling NAs
    PositiveResponse.n = as.numeric(factor(PositiveResponse, levels = c("negative", "positive"))) - 1
  ) %>% 
  group_by(Word) %>% 
  summarize(PropPositive = mean(PositiveResponse.n, na.rm = TRUE))

d.fin <- inner_join(d.final.val,d.final.conc, by=c("Word"))

d.fin <- d.fin %>% 
  mutate(
    # Assign ConcValStrict based on proportions
    ConcValStrict = case_when(
      PropConcrete > 0.75 & PropPositive > 0.75 ~ "concrete-positive",
      PropConcrete > 0.75 & PropPositive < 0.25 ~ "concrete-negative",
      PropConcrete < 0.25 & PropPositive > 0.75 ~ "abstract-positive",
      PropConcrete < 0.25 & PropPositive < 0.25 ~ "abstract-negative",
      TRUE ~ NA_character_
    )
  )

# View final result
table(d.fin$ConcValStrict)


d.fin %>%
  group_by(ConcValStrict) %>%
  summarise(Word_List = paste(Word, collapse = ", ")) %>%
  pivot_wider(names_from = ConcValStrict, values_from = Word_List) %>%
  knitr::kable() %>% 
  kable_styling(full_width = TRUE, position = "center", bootstrap_options = c("striped", "hover"))


View(d.fin)

```

```{r,fig.width=15, fig.height=5}
dodge = position_dodge(.9)
ggplot(data=d.fin, aes(x=reorder(Word,PropConcrete),y=PropConcrete,fill=ConcValStrict)) +
  geom_bar(position=dodge,stat="identity") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Recode by hand: 

```{r,fig.width=15, fig.height=5}
dodge = position_dodge(.9)
ggplot(data=d.fin, aes(x=reorder(Word,PropPositive),y=PropPositive,fill=ConcValStrict)) +
  geom_bar(position=dodge,stat="identity") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```










































# Words with the smalles difference between MeanAccuracy for Task


If we neutralize the difference between the two two accuracy, we can control for confounds with RT

```{r}

# Calculate the difference between Accuracy in Concrete and Valence
diff <- dd.fin %>%
  # filter(Word %in% d.final$Word) %>% 
  # filter(!Word %in% words_with_prefixes$Word) %>% 
  filter(!is.na(Accuracy)) %>%  # Remove rows with missing Accuracy
  group_by(Word, Task) %>%
  summarise(MeanAccuracy = mean(Accuracy)) %>%  # Calculate mean Accuracy for each group
  spread(Task, MeanAccuracy) %>%
  # spread(key = ConcValCombo, value = MeanAccuracy) %>%  # Spread the data so Concrete and Valence are separate columns
  mutate(Difference = abs(Concrete - Valence)) %>%  # Calculate the absolute difference
  arrange(Difference)  # Arrange by smallest difference
  # head(40)  # Get the top 10 words with the smallest difference

# add back in concvalcombo
diff <- inner_join(diff, d.final, by ="Word")[,c("Word","ConcValCombo","Difference","Concrete","Valence")] %>% distinct()

table(diff$ConcValCombo)

diff %>%
  group_by(ConcValCombo) %>%
  summarise(Word_List = paste(Word, collapse = ", ")) %>%
  pivot_wider(names_from = ConcValCombo, values_from = Word_List) %>%
  knitr::kable() %>% 
  kable_styling(full_width = TRUE, position = "center", bootstrap_options = c("striped", "hover"))
```



```{r,fig.width=15, fig.height=5}
dodge = position_dodge(.9)
ggplot(data=diff, aes(x=reorder(Word,Difference),y=Difference,fill=ConcValCombo)) +
  geom_bar(position=dodge,stat="identity") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


















# Choice: Words that minimizing the difference in MeanAccuracy between Task with maximizing overall mean accuracy

## Plot Mean Accuracy by Distance between Mean Accuracy grouped by Task

```{r,fig.width=15, fig.height=10}
diff <- diff %>% 
  mutate(MeanAccuracy = .5 * Concrete + .5 * Valence)

ggplot(diff, aes(x = Difference, y = MeanAccuracy)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  # geom_text(aes(label = Word), vjust = -0.5, hjust = 1.5)  # Add labels
  geom_text(aes(label = Word, color = ConcValCombo), vjust = -0.5, hjust = 1.5)  # Labels colored by ConcValCombo

```


```{r}
# Step 1: Find the maximum MeanAccuracy value
max_accuracy <- max(diff$MeanAccuracy, na.rm = TRUE)

# Step 2: Filter the words with the maximum MeanAccuracy
max_accuracy_words <- diff %>%
  filter(MeanAccuracy == max_accuracy)

# Step 3: Find the minimum Difference value among the filtered words
min_difference <- min(max_accuracy_words$Difference, na.rm = TRUE)

# Step 4: Filter the words that have both maximum MeanAccuracy and minimum Difference
result_words <- max_accuracy_words %>%
  filter(Difference == min_difference)

# Print the result
print(result_words$Word)

```


```{r}
# Select top 5 words with the highest MeanAccuracy
top_accuracy_words <- diff %>%
  arrange(desc(MeanAccuracy)) 
# %>%
  head(50)

# Select top 5 words with the lowest Difference
bottom_difference_words <- top_accuracy_words %>%
  arrange(Difference) 
# %>%
  head(45)

# Print the top and bottom words
# print(top_accuracy_words)
print(bottom_difference_words,n=40)

table(bottom_difference_words$ConcValCombo)
length(unique(bottom_difference_words$Word))
```


## For these words, what are the Quantitative results?

```{r}
sub.data <- inner_join(d.final,bottom_difference_words)
  
ggplot(sub.data, aes(LogReactionTime, fill=Task)) +
  geom_density(alpha = .5)

summary(d.final$LogReactionTime)

table(sub.data$ConcValCombo)

# write.csv(sub.data,"../data/best_words.csv")
```

### Accuracy Overall (not removing innacurate trials)
```{r,fig.width=10, fig.height=5}
agr <- sub.data %>% 
  group_by(Task) %>% 
  reframe(MeanAccuracy = mean(Accuracy), 
          CILow = ci.low(Accuracy), 
          CIHigh = ci.high(Accuracy)) %>%
  mutate(YMin = MeanAccuracy - CILow, 
         YMax = MeanAccuracy + CIHigh)

dodge = position_dodge(.9)
ggplot(data=agr, aes(x=Task,y=MeanAccuracy, fill=Task)) +
  geom_bar(position=dodge,stat="identity") + 
  geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.25,position=position_dodge(0.9)) +
  guides(fill = "none")

```

### Mean Accuracy by Word / Task
```{r,fig.width=10, fig.height=10}
agr <- sub.data %>%
  group_by(Task,Word) %>% 
  mutate(MeanAccuracy = mean(Accuracy), 
          CILow = ci.low(Accuracy), 
          CIHigh = ci.high(Accuracy)) %>%
  mutate(YMin = MeanAccuracy - CILow, 
         YMax = MeanAccuracy + CIHigh)

dodge = position_dodge(.9)
ggplot(data=agr, aes(x=Task,y=MeanAccuracy,fill=Task)) +
  geom_bar(position=dodge,stat="identity") +
  facet_wrap(~Word) +
  geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.25,position=position_dodge(0.9)) +
  guides(fill = "none")

```

### Proper, irritate, hope, embelish
```{r}
sub.data <- sub.data %>%
  mutate(Word = ifelse(Word == "embellish", "embelish", Word))
```

```{r}
names(sub.data)

agr <- sub.data %>%
  filter(Word %in% c("prosper","irritate","hope","embelish","presume")) %>% 
  filter(Task == "Concrete") %>% 
  group_by(Task,Word) %>% 
  mutate(PropConcrete = mean(Concrete), 
          CILow = ci.low(Concrete), 
          CIHigh = ci.high(Concrete)) %>%
  mutate(YMin = PropConcrete - CILow, 
         YMax = PropConcrete + CIHigh)

dodge = position_dodge(.9)
ggplot(data=agr, aes(x=Word,y=PropConcrete,fill=ConcValCombo)) +
  geom_bar(position=dodge,stat="identity") +
  # facet_wrap(~Word) +
  geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.25,position=position_dodge(0.9))
  # guides(fill = "none")
```


```{r}
agr <- sub.data %>%
  filter(Word %in% c("prosper","irritate","hope","embelish","presume")) %>% 
  filter(Task == "Valence") %>% 
  group_by(ConcValCombo,Word) %>% 
  mutate(PropPositive = mean(Valence), 
          CILow = ci.low(Valence), 
          CIHigh = ci.high(Valence)) %>%
  mutate(YMin = PropPositive - CILow, 
         YMax = PropPositive + CIHigh)

dodge = position_dodge(.9)
ggplot(data=agr, aes(x=Word,y=PropPositive,fill=ConcValCombo)) +
  geom_bar(position=dodge,stat="identity") +
  # facet_wrap(~Word) +
  geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.25,position=position_dodge(0.9))
  # guides(fill = "none")
```


# find the values of Word that have lower MeanAccuracy for Task==Concrete than for Task==Valance

Remove the two words with the biggest difference
```{r}
length(unique(sub.data$Word))
# Step 2: Filter for Words where MeanAccuracy_Concrete < MeanAccuracy_Valence
words_lower_accuracy_concrete <- sub.data %>%
  filter(Concrete < Valence) %>%
  arrange(desc(Difference)) %>%
  pull(Word)  # Extract the list of Words

print(unique(words_lower_accuracy_concrete))
# Encourage and praise

# table(sub.data$ConcValCombo)


```

# sub.list minus encourage and praise
```{r}

sub.data %>%
  group_by(ConcValCombo) %>%
  summarise(Word_List = paste(unique(Word), collapse = ", ")) %>%
  pivot_wider(names_from = ConcValCombo, values_from = Word_List) %>%
  knitr::kable() %>% 
  kable_styling(full_width = TRUE, position = "center", bootstrap_options = c("striped", "hover"))
```

```{r,fig.width=10, fig.height=10}
agr <- sub.data %>%
  group_by(Word,ConcValCombo) %>% 
  mutate(MeanDifference = mean(Difference), 
          CILow = ci.low(Difference),
          CIHigh = ci.high(Difference)) %>%
  mutate(YMin = MeanDifference - CILow,
         YMax = MeanDifference + CIHigh)

dodge = position_dodge(.9)
ggplot(data=agr, aes(x=reorder(Word,MeanDifference),y=MeanDifference,fill=ConcValCombo)) +
  geom_bar(position=dodge,stat="identity") +
  # facet_wrap(~Word) +
  geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.25,position=position_dodge(0.9)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
  # guides(fill = "none")

```


```{r}

sub.data.conc.high <- d %>% 
  # select(Word,Task,ConcValCombo) %>% 
  group_by(Word,Task) %>% 
  summarize(MeanAccuracy = mean(Accuracy)) %>%
  filter( MeanAccuracy[Task == "Concrete"] <.85)
print(sub.data.conc.high$Word)


table(sub.data.conc.high$ConcValCombo)
```



# Create final list
```{r}
sub.final <- sub.data %>% 
  # filter(!Word %in% words_with_prefixes$Word) %>% 
  filter(Word %in% sub.data.conc.high$Word) %>% 
  filter(!Word %in% c("praise","encourage")) %>% 
  select(Word, ConcValCombo) %>% 
  distinct() 

table(sub.final$ConcValCombo)


sub.final %>% 
  group_by(ConcValCombo) %>%
  summarise(Word_List = paste(unique(Word), collapse = ", ")) %>%
  pivot_wider(names_from = ConcValCombo, values_from = Word_List) %>%
  knitr::kable() %>% 
  kable_styling(full_width = TRUE, position = "center", bootstrap_options = c("striped", "hover"))

```

## Look only at words abstract-negative not already in sub.final

```{r}
an <- d %>% 
  filter(!Word %in% words_with_prefixes$Word) %>% 
  filter((ConcValCombo == "abstract-negative") &
         (!Word %in% sub.final$Word))  %>% 
  group_by(Word,Task) %>% 
  mutate(MeanAccuracy = mean(Accuracy), 
          CILow = ci.low(Accuracy), 
          CIHigh = ci.high(Accuracy)) %>%
  mutate(YMin = MeanAccuracy - CILow, 
         YMax = MeanAccuracy + CIHigh)

dodge = position_dodge(.9)
ggplot(data=an, aes(x=Task,y=MeanAccuracy,fill=Task)) +
  geom_bar(position=dodge,stat="identity") +
  facet_wrap(~Word) +
  geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.25,position=position_dodge(0.9)) +
  guides(fill = "none")



```


## Plot diff by Accuracy
```{r}
agr <- d %>% 
  filter(!Word %in% words_with_prefixes$Word) %>% 
  filter((ConcValCombo == "abstract-negative")&
           (!Word %in% sub.final$Word))  %>% 
  group_by(Word, Task) %>%
  summarise(MeanAccuracy = mean(Accuracy)) %>%  # Calculate mean Accuracy for each group
  spread(Task, MeanAccuracy) %>%
  # spread(key = ConcValCombo, value = MeanAccuracy) %>%  # Spread the data so Concrete and Valence are separate columns
  mutate(Difference = abs(Concrete - Valence)) %>%
  mutate(MeanAccuracy = .5 * Concrete + .5 * Valence)

ggplot(agr, aes(x = Difference, y = MeanAccuracy)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  # geom_text(aes(label = Word), vjust = -0.5, hjust = 1.5)  # Add labels
  geom_text(aes(label = Word), vjust = -0.5, hjust = 1.5)  # Labels colored by ConcValCombo
```

### take a look at annoy, violate, distrust, irritate
```{r}


sub <- d %>% 
  filter(Word %in% c("annoy", "violate", "irritate","manipulate")) %>% 
  filter(Task == "Concrete") %>% 
  mutate(Response.n = as.numeric(factor(Response, levels = c("abstract", "concrete"))) - 1) %>%
  group_by(Word) %>%
  mutate(PropConcrete = mean(Response.n), 
          CILow = ci.low(Response.n),
          CIHigh = ci.high(Response.n)) %>%
  mutate(YMin = PropConcrete - CILow,
         YMax = PropConcrete + CIHigh)

dodge = position_dodge(.9)
ggplot(data=sub, aes(x=reorder(Word,PropConcrete),y=PropConcrete,fill=Word)) +
  geom_bar(position=dodge,stat="identity") +
  geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.25,position=position_dodge(0.9))


agr <- d %>% 
  filter(Word %in% c("annoy", "violate", "irritate","manipulate")) %>% 
  filter(!is.na(Accuracy)) %>%  # Remove rows with missing Accuracy
  group_by(Word, Task) %>%
  mutate(Response.n = as.numeric(factor(Response, levels = c("abstract", "concrete"))) - 1) %>%
  summarise(MeanAccuracy = mean(Accuracy)) %>%  # Calculate mean Accuracy for each group
  spread(Task, MeanAccuracy) %>%
  # spread(key = ConcValCombo, value = MeanAccuracy) %>%  # Spread the data so Concrete and Valence are separate columns
  mutate(Difference = abs(Concrete - Valence)) 


ggplot(agr, aes(x = Difference, y = Concrete)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  # geom_text(aes(label = Word), vjust = -0.5, hjust = 1.5)  # Add labels
  geom_text(aes(label = Word), vjust = -0.5, hjust = 1.5)  # Labels colored by ConcValCombo

```









# Start removing words 

```{r}
sub.final <- d %>% 
  # Remove words with prefixes that have a negative denotation/connotation
  filter(!Word %in% words_with_prefixes$Word) %>% 
  filter(
    (!Word %in% c("praise", "encourage") & Word %in% sub.data$Word) |
      Word %in% c("annoy", "violate")
  ) %>% 
  select(Word, ConcValCombo) %>% 
  distinct()

# View(d[c(d$Word == "annoy"),])
length(unique(sub.final$Word))
table(sub.final$ConcValCombo)


sub.final %>% 
  group_by(ConcValCombo) %>%
  summarise(Word_List = paste(unique(Word), collapse = ", ")) %>%
  pivot_wider(names_from = ConcValCombo, values_from = Word_List) %>%
  knitr::kable() %>% 
  kable_styling(full_width = TRUE, position = "center", bootstrap_options = c("striped", "hover"))

```

### Two extra in abstract-positive","concrete-negative
remove by chozing max diff and min mean acc
```{r}

agr <- d %>% 
  filter(Word %in% sub.final$Word) %>% 
  filter(ConcValCombo %in% c("abstract-positive","concrete-negative")) %>% 
  filter(!is.na(Accuracy)) %>%  # Remove rows with missing Accuracy
  group_by(Word, Task,ConcValCombo) %>%
  mutate(Response.n = as.numeric(factor(Response, levels = c("abstract", "concrete"))) - 1) %>%
  summarise(MeanAccuracy = mean(Accuracy)) %>%  # Calculate mean Accuracy for each group
  spread(Task, MeanAccuracy) %>%
  # spread(key = ConcValCombo, value = MeanAccuracy) %>%  # Spread the data so Concrete and Valence are separate columns
  mutate(Difference = abs(Concrete - Valence),
         MeanAccuracy = .5*Concrete + .5*Valence) 

ggplot(agr, aes(x = Difference, y = MeanAccuracy)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  # geom_text(aes(label = Word), vjust = -0.5, hjust = 1.5)  # Add labels
  geom_text(aes(label = Word,color=ConcValCombo), vjust = -0.5, hjust = 1.5)  # Labels colored by ConcValCombo

# admire and puke remove
```


### Remove admire and puke

```{r}

sub <- d %>% 
  filter(Word %in% c("admire","fulfill","puke","die")) %>% 
  filter(Task == "Concrete") %>% 
  mutate(Response.n = as.numeric(factor(Response, levels = c("abstract", "concrete"))) - 1) %>%
  group_by(Word,ConcValCombo) %>%
  mutate(PropConcrete = mean(Response.n), 
          CILow = ci.low(Response.n),
          CIHigh = ci.high(Response.n)) %>%
  mutate(YMin = PropConcrete - CILow,
         YMax = PropConcrete + CIHigh)

dodge = position_dodge(.9)
ggplot(data=sub, aes(x=reorder(Word,PropConcrete),y=PropConcrete,fill=ConcValCombo)) +
  geom_bar(position=dodge,stat="identity") +
  geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.25,position=position_dodge(0.9))


```




```{r}
sub.final.final <- d %>% 
  filter(!Word %in% words_with_prefixes$Word) %>% 
  filter(
    (Word %in% sub.data$Word &
       !Word %in% c("praise", "encourage", "distrust", "disappoint","fulfill","puke") ) |
       Word %in% c("annoy", "violate")
      ) %>% 
  select(Word, ConcValCombo) %>% 
  # filter( !Word %in% c()  )
  distinct()

table(sub.final.final$ConcValCombo)


sub.final.final %>% 
  group_by(ConcValCombo) %>%
  summarise(Word_List = paste(unique(Word), collapse = ", ")) %>%
  pivot_wider(names_from = ConcValCombo, values_from = Word_List) %>%
  knitr::kable() %>% 
  kable_styling(full_width = TRUE, position = "center", bootstrap_options = c("striped", "hover"))



```





```{r}
# write.csv(sub.final.final,"../data/pilot4_words.csv")

sub.final.final %>% 
  group_by(ConcValCombo) %>%
  summarise(Word_List = paste(unique(Word), collapse = ", ")) %>%
  pivot_wider(names_from = ConcValCombo, values_from = Word_List) %>%
  knitr::kable() %>% 
  kable_styling(full_width = TRUE, position = "center", bootstrap_options = c("striped", "hover"))

table(sub.final.final$ConcValCombo)
```




















### Reaction Time

```{r,fig.width=10, fig.height=5}

agr <- d %>%
    filter(Word %in% sub.final.final$Word) %>% 
    group_by(Task,Word) %>%
    summarize(MeanLogReactionTime = mean(LogReactionTime), 
              CILow = ci.low(LogReactionTime), 
              CIHigh = ci.high(LogReactionTime)) %>%
    mutate(YMin = MeanLogReactionTime - CILow, 
           YMax = MeanLogReactionTime + CIHigh)

ggplot(agr, aes(x=MeanLogReactionTime, fill=Task)) +
  geom_density(alpha = .4)

ggplot(agr, aes(x=Task, y=MeanLogReactionTime,fill=Task)) + 
    geom_violin(trim=FALSE,alpha=.4) +
    geom_jitter(shape=16, position=position_jitter(0.2)) +
    guides(fill = "none")

```

# By Item

```{r,fig.width=10, fig.height=10}
agr <- d %>%
    filter(Word %in% sub.final.final$Word) %>% 
    group_by(Task,Word) %>%
    summarize(MeanReactionTime = mean(ReactionTime), 
              CILow = ci.low(ReactionTime), 
              CIHigh = ci.high(ReactionTime)) %>%
    mutate(YMin = MeanReactionTime - CILow, 
           YMax = MeanReactionTime + CIHigh)

dodge = position_dodge(.9)
ggplot(data=agr, aes(x=Task,y=MeanReactionTime,fill=Task)) +
  geom_bar(position=dodge,stat="identity") +
  facet_wrap(~Word) +
  geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.25,position=position_dodge(0.9)) +
  guides(fill = "none")
```


```{r}
summary(sub.data$ReactionTime)

agr <- d %>%
  filter(Word %in% sub.final.final$Word) %>% 
  group_by(Task) %>% 
  summarise(MeanRT = mean(ReactionTime)) %>%                                                            
  print()

```

# By Item

```{r,fig.width=10, fig.height=10}
agr <- d %>%
    filter(Word %in% sub.final.final$Word) %>% 
     mutate(ConcValCombo = case_when(
      Word == "hope" ~ "concrete-positive",
      Word == "violate" ~ "concrete-negative",
      TRUE ~ ConcValCombo  # Keep original value if condition is not met
    )) %>% 
    group_by(Task,Word,ConcValCombo) %>%
    summarize(MeanReactionTime = mean(ReactionTime), 
              CILow = ci.low(ReactionTime), 
              CIHigh = ci.high(ReactionTime)) %>%
    mutate(YMin = MeanReactionTime - CILow, 
           YMax = MeanReactionTime + CIHigh)

dodge = position_dodge(.9)
ggplot(data=agr, aes(x=Task,y=MeanReactionTime,fill=ConcValCombo)) +
  geom_bar(position=dodge,stat="identity") +
  facet_wrap(~Word) +
  geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.25,position=position_dodge(0.9))
  # guides(fill = "none")
```


# Analysis
```{r, include=FALSE, warning=FALSE, echo=FALSE}
str(sub.data)

convert_except <- function(df, exclude_cols) {
  df[] <- lapply(names(df), function(col_name) {
    if (!(col_name %in% exclude_cols)) {
      return(as.factor(df[[col_name]]))  # Convert to factor if not in excluded columns
    } else {
      return(df[[col_name]])  # Leave the excluded columns unchanged
    }
  })
  return(df)
}
# Specify columns to exclude from conversion
exclude_columns <- c('ReactionTime', 'LogReactionTime')

# Convert all columns to factors except for 'a' and 'd'
df_factors <- convert_except(sub.data, exclude_columns)

# Check the structure of the modified data frame
str(df_factors)

```

```{r, include=FALSE, warning=FALSE, echo=FALSE}
contrasts(df_factors$ConcValCombo)
contrasts(df_factors$Task)

center = df_factors %>%
  mutate(cTask = as.numeric(Task)-mean(as.numeric(Task)),
         cAccuracy = as.numeric(Accuracy)-mean(as.numeric(Accuracy)),
         cBlockOrder = as.numeric(BlockOrder)-mean(as.numeric(BlockOrder))
         )
  
  # droplevels()
```

# Is there a difference between Semantic and Valence Tasks?
Yes.

```{r}

m = lmer(LogReactionTime ~ cTask + (1+cTask|ID.true) + (1+cTask|Word), data=center)

summary(m)
```


# Does Accuracy on Task predict reaction time? 
No, but there is a main effect of accuracy on RT, but no interaction
```{r}

m = lmer(LogReactionTime ~ cAccuracy*cTask + (1+cTask|ID.true) + (1+cTask|Word), data=center)

summary(m)
```



## Removing inaccurate trials

It doesn't make sense 

```{r}
sub.data.acc.trials.only <- sub.data %>% 
  group_by(ID.true) %>% 
  filter(Accuracy == 1)

nrow(sub.data.acc.trials.only)/nrow(sub.data)*100
```


### Reaction Time

```{r,fig.width=10, fig.height=5}

agr = sub.data.acc.trials.only %>%
    group_by(Task,Word) %>%
    summarize(MeanLogReactionTime = mean(LogReactionTime), 
              CILow = ci.low(LogReactionTime), 
              CIHigh = ci.high(LogReactionTime)) %>%
    mutate(YMin = MeanLogReactionTime - CILow, 
           YMax = MeanLogReactionTime + CIHigh)

ggplot(agr, aes(x=MeanLogReactionTime, fill=Task)) +
  geom_density(alpha = .4)

ggplot(agr, aes(x=Task, y=MeanLogReactionTime,fill=Task)) + 
    geom_violin(trim=FALSE,alpha=.4) +
    geom_jitter(shape=16, position=position_jitter(0.2)) +
    guides(fill = "none")

```

# By Item

```{r,fig.width=10, fig.height=10}
agr = sub.data.acc.trials.only %>%
    group_by(Task,Word) %>%
    summarize(MeanReactionTime = mean(ReactionTime), CILow = ci.low(ReactionTime), CIHigh = ci.high(ReactionTime)) %>%
    mutate(YMin = MeanReactionTime - CILow, YMax = MeanReactionTime + CIHigh)

dodge = position_dodge(.9)
ggplot(data=agr, aes(x=Task,y=MeanReactionTime,fill=Task)) +
  geom_bar(position=dodge,stat="identity") +
  facet_wrap(~Word) +
  geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.25,position=position_dodge(0.9)) +
  guides(fill = "none")
```

# Analysis
```{r, include=FALSE, warning=FALSE, echo=FALSE}
# str(sub.data.acc.trials.only)

# head(sub.data.acc.trials.only)

convert_except <- function(df, exclude_cols) {
  df[] <- lapply(names(df), function(col_name) {
    if (!(col_name %in% exclude_cols)) {
      return(as.factor(df[[col_name]]))  # Convert to factor if not in excluded columns
    } else {
      return(df[[col_name]])  # Leave the excluded columns unchanged
    }
  })
  return(df)
}
# Specify columns to exclude from conversion
exclude_columns <- c('ReactionTime', 'LogReactionTime')

# Convert all columns to factors except for 'a' and 'd'
df_factors <- convert_except(sub.data.acc.trials.only, exclude_columns)

# Check the structure of the modified data frame
# str(df_factors)

```

```{r, include=FALSE, warning=FALSE, echo=FALSE}
contrasts(df_factors$ConcValCombo)
contrasts(df_factors$Task)

center = df_factors %>%
  mutate(cTask = as.numeric(Task)-mean(as.numeric(Task)),
         cAccuracy = as.numeric(Accuracy)-mean(as.numeric(Accuracy)),
         cBlockOrder = as.numeric(BlockOrder)-mean(as.numeric(BlockOrder))
         )
  
  # droplevels()
```

# Is there a difference between Semantic and Valence Tasks?
Yes.

```{r}

m = lmer(LogReactionTime ~ cTask + (1+cTask|ID.true) + (1+cTask|Word), data=center)

summary(m)
```


# Does Accuracy on Task predict reaction time? 
No.
```{r}

m = lmer(LogReactionTime ~ ConcValCombo*cTask + (1+cTask+ConcValCombo|ID.true) + (1+cTask|Word), data=center)

summary(m)
```
